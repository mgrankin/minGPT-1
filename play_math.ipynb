{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT on addition\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "    encoding will simply be the n-digit first number, n-digit second number, \n",
    "    and (n+1)-digit result, all simply concatenated together. Because each addition\n",
    "    problem is so structured, there is no need to bother the model with encoding\n",
    "    +, =, or other tokens. Each possible sequence has the same length, and simply\n",
    "    contains the raw digits of the addition problem.\n",
    "    \n",
    "    As a few examples, the 2-digit problems:\n",
    "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
    "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
    "    etc.\n",
    "    \n",
    "    We will also only train GPT on the final (n+1)-digits because the first\n",
    "    two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
    "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
    "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
    "    in 3 sequential steps.\n",
    "    \n",
    "    fun exercise: does it help if the result is asked to be produced in reverse order?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndigit, split):\n",
    "        self.split = split # train/test\n",
    "        self.ndigit = ndigit\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset for e.g. 2-digit addition\n",
    "ndigit = 2\n",
    "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
    "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0] # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2021 14:18:12 - INFO - mingpt.model -   number of parameters: 4.001280e+05\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "\n",
    "#98.80%\n",
    "\n",
    "#99.07%\n",
    "#99.50%\n",
    "#embd_pdrop = 0.0, \n",
    "\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=2, n_head=4, n_embd=128,\n",
    "                 # embd_pdrop = 0.0, \n",
    "                 # resid_pdrop = 0.0, \n",
    "                 # attn_pdrop = 0.0,\n",
    "                 )\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/home/u/.anaconda/envs/mingpt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 17: train loss 1.96685. lr 5.999848e-04: 100%|██████████| 18/18 [00:09<00:00,  1.88it/s]\n",
      "12/01/2021 14:18:25 - INFO - mingpt.trainer -   test loss: 1.836349\n",
      "epoch 2 iter 17: train loss 1.76151. lr 5.999367e-04: 100%|██████████| 18/18 [00:00<00:00, 28.43it/s]\n",
      "12/01/2021 14:18:27 - INFO - mingpt.trainer -   test loss: 1.637177\n",
      "epoch 3 iter 17: train loss 1.63700. lr 5.998557e-04: 100%|██████████| 18/18 [00:00<00:00, 24.70it/s]\n",
      "12/01/2021 14:18:28 - INFO - mingpt.trainer -   test loss: 1.489183\n",
      "epoch 4 iter 17: train loss 1.54155. lr 5.997417e-04: 100%|██████████| 18/18 [00:00<00:00, 27.08it/s]\n",
      "12/01/2021 14:18:30 - INFO - mingpt.trainer -   test loss: 1.361760\n",
      "epoch 5 iter 17: train loss 1.48659. lr 5.995950e-04: 100%|██████████| 18/18 [00:00<00:00, 26.34it/s]\n",
      "12/01/2021 14:18:31 - INFO - mingpt.trainer -   test loss: 1.287460\n",
      "epoch 6 iter 17: train loss 1.39373. lr 5.994153e-04: 100%|██████████| 18/18 [00:00<00:00, 26.64it/s]\n",
      "12/01/2021 14:18:32 - INFO - mingpt.trainer -   test loss: 1.201919\n",
      "epoch 7 iter 17: train loss 1.35667. lr 5.992028e-04: 100%|██████████| 18/18 [00:00<00:00, 27.51it/s]\n",
      "12/01/2021 14:18:34 - INFO - mingpt.trainer -   test loss: 1.149651\n",
      "epoch 8 iter 17: train loss 1.30843. lr 5.989575e-04: 100%|██████████| 18/18 [00:00<00:00, 27.81it/s]\n",
      "12/01/2021 14:18:35 - INFO - mingpt.trainer -   test loss: 1.108286\n",
      "epoch 9 iter 17: train loss 1.27358. lr 5.986794e-04: 100%|██████████| 18/18 [00:00<00:00, 27.33it/s]\n",
      "12/01/2021 14:18:36 - INFO - mingpt.trainer -   test loss: 1.078489\n",
      "epoch 10 iter 17: train loss 1.26861. lr 5.983686e-04: 100%|██████████| 18/18 [00:00<00:00, 24.65it/s]\n",
      "12/01/2021 14:18:38 - INFO - mingpt.trainer -   test loss: 1.047325\n",
      "epoch 11 iter 17: train loss 1.22736. lr 5.980250e-04: 100%|██████████| 18/18 [00:00<00:00, 27.14it/s]\n",
      "12/01/2021 14:18:39 - INFO - mingpt.trainer -   test loss: 1.028363\n",
      "epoch 12 iter 17: train loss 1.19950. lr 5.976487e-04: 100%|██████████| 18/18 [00:00<00:00, 26.52it/s]\n",
      "12/01/2021 14:18:41 - INFO - mingpt.trainer -   test loss: 1.009358\n",
      "epoch 13 iter 17: train loss 1.19365. lr 5.972398e-04: 100%|██████████| 18/18 [00:00<00:00, 26.92it/s]\n",
      "12/01/2021 14:18:42 - INFO - mingpt.trainer -   test loss: 0.997442\n",
      "epoch 14 iter 17: train loss 1.16289. lr 5.967983e-04: 100%|██████████| 18/18 [00:00<00:00, 26.64it/s]\n",
      "12/01/2021 14:18:43 - INFO - mingpt.trainer -   test loss: 0.955628\n",
      "epoch 15 iter 17: train loss 1.06551. lr 5.963242e-04: 100%|██████████| 18/18 [00:00<00:00, 26.26it/s]\n",
      "12/01/2021 14:18:45 - INFO - mingpt.trainer -   test loss: 0.851939\n",
      "epoch 16 iter 17: train loss 0.99007. lr 5.958176e-04: 100%|██████████| 18/18 [00:00<00:00, 26.89it/s]\n",
      "12/01/2021 14:18:46 - INFO - mingpt.trainer -   test loss: 0.697099\n",
      "epoch 17 iter 17: train loss 0.87294. lr 5.952786e-04: 100%|██████████| 18/18 [00:00<00:00, 24.98it/s]\n",
      "12/01/2021 14:18:48 - INFO - mingpt.trainer -   test loss: 0.588012\n",
      "epoch 18 iter 17: train loss 0.79932. lr 5.947071e-04: 100%|██████████| 18/18 [00:00<00:00, 26.34it/s]\n",
      "12/01/2021 14:18:49 - INFO - mingpt.trainer -   test loss: 0.458841\n",
      "epoch 19 iter 17: train loss 0.72794. lr 5.941034e-04: 100%|██████████| 18/18 [00:00<00:00, 26.51it/s]\n",
      "12/01/2021 14:18:51 - INFO - mingpt.trainer -   test loss: 0.399684\n",
      "epoch 20 iter 17: train loss 0.68212. lr 5.934674e-04: 100%|██████████| 18/18 [00:00<00:00, 26.74it/s]\n",
      "12/01/2021 14:18:52 - INFO - mingpt.trainer -   test loss: 0.339062\n",
      "epoch 21 iter 17: train loss 0.63987. lr 5.927992e-04: 100%|██████████| 18/18 [00:00<00:00, 27.30it/s]\n",
      "12/01/2021 14:18:53 - INFO - mingpt.trainer -   test loss: 0.319047\n",
      "epoch 22 iter 17: train loss 0.59973. lr 5.920989e-04: 100%|██████████| 18/18 [00:00<00:00, 26.80it/s]\n",
      "12/01/2021 14:18:55 - INFO - mingpt.trainer -   test loss: 0.286085\n",
      "epoch 23 iter 17: train loss 0.56223. lr 5.913665e-04: 100%|██████████| 18/18 [00:00<00:00, 25.28it/s]\n",
      "12/01/2021 14:18:56 - INFO - mingpt.trainer -   test loss: 0.257465\n",
      "epoch 24 iter 17: train loss 0.56020. lr 5.906022e-04: 100%|██████████| 18/18 [00:00<00:00, 24.61it/s]\n",
      "12/01/2021 14:18:58 - INFO - mingpt.trainer -   test loss: 0.231016\n",
      "epoch 25 iter 17: train loss 0.48340. lr 5.898060e-04: 100%|██████████| 18/18 [00:00<00:00, 26.55it/s]\n",
      "12/01/2021 14:18:59 - INFO - mingpt.trainer -   test loss: 0.212512\n",
      "epoch 26 iter 17: train loss 0.48669. lr 5.889780e-04: 100%|██████████| 18/18 [00:00<00:00, 27.44it/s]\n",
      "12/01/2021 14:19:00 - INFO - mingpt.trainer -   test loss: 0.191924\n",
      "epoch 27 iter 17: train loss 0.45128. lr 5.881183e-04: 100%|██████████| 18/18 [00:00<00:00, 25.08it/s]\n",
      "12/01/2021 14:19:02 - INFO - mingpt.trainer -   test loss: 0.170767\n",
      "epoch 28 iter 17: train loss 0.44680. lr 5.872271e-04: 100%|██████████| 18/18 [00:00<00:00, 26.19it/s]\n",
      "12/01/2021 14:19:03 - INFO - mingpt.trainer -   test loss: 0.151401\n",
      "epoch 29 iter 17: train loss 0.46026. lr 5.863043e-04: 100%|██████████| 18/18 [00:00<00:00, 25.93it/s]\n",
      "12/01/2021 14:19:05 - INFO - mingpt.trainer -   test loss: 0.144555\n",
      "epoch 30 iter 17: train loss 0.41325. lr 5.853501e-04: 100%|██████████| 18/18 [00:00<00:00, 25.42it/s]\n",
      "12/01/2021 14:19:06 - INFO - mingpt.trainer -   test loss: 0.133005\n",
      "epoch 31 iter 17: train loss 0.41109. lr 5.843646e-04: 100%|██████████| 18/18 [00:00<00:00, 26.55it/s]\n",
      "12/01/2021 14:19:07 - INFO - mingpt.trainer -   test loss: 0.112682\n",
      "epoch 32 iter 17: train loss 0.38212. lr 5.833479e-04: 100%|██████████| 18/18 [00:00<00:00, 26.99it/s]\n",
      "12/01/2021 14:19:09 - INFO - mingpt.trainer -   test loss: 0.095564\n",
      "epoch 33 iter 17: train loss 0.35982. lr 5.823001e-04: 100%|██████████| 18/18 [00:00<00:00, 26.37it/s]\n",
      "12/01/2021 14:19:10 - INFO - mingpt.trainer -   test loss: 0.098505\n",
      "epoch 34 iter 17: train loss 0.37664. lr 5.812214e-04: 100%|██████████| 18/18 [00:00<00:00, 27.12it/s]\n",
      "12/01/2021 14:19:12 - INFO - mingpt.trainer -   test loss: 0.088332\n",
      "epoch 35 iter 17: train loss 0.35063. lr 5.801118e-04: 100%|██████████| 18/18 [00:00<00:00, 26.27it/s]\n",
      "12/01/2021 14:19:13 - INFO - mingpt.trainer -   test loss: 0.077969\n",
      "epoch 36 iter 17: train loss 0.31417. lr 5.789715e-04: 100%|██████████| 18/18 [00:00<00:00, 26.55it/s]\n",
      "12/01/2021 14:19:14 - INFO - mingpt.trainer -   test loss: 0.073985\n",
      "epoch 37 iter 17: train loss 0.32517. lr 5.778006e-04: 100%|██████████| 18/18 [00:00<00:00, 26.45it/s]\n",
      "12/01/2021 14:19:16 - INFO - mingpt.trainer -   test loss: 0.064826\n",
      "epoch 38 iter 17: train loss 0.30539. lr 5.765993e-04: 100%|██████████| 18/18 [00:00<00:00, 24.54it/s]\n",
      "12/01/2021 14:19:17 - INFO - mingpt.trainer -   test loss: 0.063355\n",
      "epoch 39 iter 17: train loss 0.30795. lr 5.753675e-04: 100%|██████████| 18/18 [00:00<00:00, 27.13it/s]\n",
      "12/01/2021 14:19:19 - INFO - mingpt.trainer -   test loss: 0.055703\n",
      "epoch 40 iter 17: train loss 0.32147. lr 5.741056e-04: 100%|██████████| 18/18 [00:00<00:00, 26.66it/s]\n",
      "12/01/2021 14:19:20 - INFO - mingpt.trainer -   test loss: 0.049654\n",
      "epoch 41 iter 17: train loss 0.31423. lr 5.728136e-04: 100%|██████████| 18/18 [00:00<00:00, 27.58it/s]\n",
      "12/01/2021 14:19:21 - INFO - mingpt.trainer -   test loss: 0.047181\n",
      "epoch 42 iter 17: train loss 0.30316. lr 5.714917e-04: 100%|██████████| 18/18 [00:00<00:00, 27.13it/s]\n",
      "12/01/2021 14:19:23 - INFO - mingpt.trainer -   test loss: 0.043078\n",
      "epoch 43 iter 17: train loss 0.26152. lr 5.701400e-04: 100%|██████████| 18/18 [00:00<00:00, 26.85it/s]\n",
      "12/01/2021 14:19:24 - INFO - mingpt.trainer -   test loss: 0.033966\n",
      "epoch 44 iter 17: train loss 0.26247. lr 5.687587e-04: 100%|██████████| 18/18 [00:00<00:00, 26.58it/s]\n",
      "12/01/2021 14:19:26 - INFO - mingpt.trainer -   test loss: 0.033876\n",
      "epoch 45 iter 17: train loss 0.26142. lr 5.673479e-04: 100%|██████████| 18/18 [00:00<00:00, 26.30it/s]\n",
      "12/01/2021 14:19:27 - INFO - mingpt.trainer -   test loss: 0.033753\n",
      "epoch 46 iter 17: train loss 0.26138. lr 5.659078e-04: 100%|██████████| 18/18 [00:00<00:00, 23.18it/s]\n",
      "12/01/2021 14:19:28 - INFO - mingpt.trainer -   test loss: 0.031806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 47 iter 17: train loss 0.24275. lr 5.644385e-04: 100%|██████████| 18/18 [00:00<00:00, 25.90it/s]\n",
      "12/01/2021 14:19:30 - INFO - mingpt.trainer -   test loss: 0.028594\n",
      "epoch 48 iter 17: train loss 0.22470. lr 5.629402e-04: 100%|██████████| 18/18 [00:00<00:00, 26.86it/s]\n",
      "12/01/2021 14:19:31 - INFO - mingpt.trainer -   test loss: 0.023940\n",
      "epoch 49 iter 17: train loss 0.24933. lr 5.614131e-04: 100%|██████████| 18/18 [00:00<00:00, 26.59it/s]\n",
      "12/01/2021 14:19:33 - INFO - mingpt.trainer -   test loss: 0.023341\n",
      "epoch 50 iter 17: train loss 0.22519. lr 5.598573e-04: 100%|██████████| 18/18 [00:00<00:00, 26.80it/s]\n",
      "12/01/2021 14:19:34 - INFO - mingpt.trainer -   test loss: 0.019271\n",
      "epoch 51 iter 17: train loss 0.23255. lr 5.582729e-04: 100%|██████████| 18/18 [00:00<00:00, 25.47it/s]\n",
      "12/01/2021 14:19:35 - INFO - mingpt.trainer -   test loss: 0.024150\n",
      "epoch 52 iter 17: train loss 0.21736. lr 5.566603e-04: 100%|██████████| 18/18 [00:00<00:00, 27.36it/s]\n",
      "12/01/2021 14:19:37 - INFO - mingpt.trainer -   test loss: 0.019122\n",
      "epoch 53 iter 17: train loss 0.22520. lr 5.550195e-04: 100%|██████████| 18/18 [00:00<00:00, 24.36it/s]\n",
      "12/01/2021 14:19:38 - INFO - mingpt.trainer -   test loss: 0.015422\n",
      "epoch 54 iter 17: train loss 0.22973. lr 5.533507e-04: 100%|██████████| 18/18 [00:00<00:00, 26.76it/s]\n",
      "12/01/2021 14:19:40 - INFO - mingpt.trainer -   test loss: 0.016732\n",
      "epoch 55 iter 17: train loss 0.21790. lr 5.516542e-04: 100%|██████████| 18/18 [00:00<00:00, 26.70it/s]\n",
      "12/01/2021 14:19:41 - INFO - mingpt.trainer -   test loss: 0.014522\n",
      "epoch 56 iter 17: train loss 0.19216. lr 5.499300e-04: 100%|██████████| 18/18 [00:00<00:00, 25.60it/s]\n",
      "12/01/2021 14:19:43 - INFO - mingpt.trainer -   test loss: 0.014301\n",
      "epoch 57 iter 17: train loss 0.22818. lr 5.481784e-04: 100%|██████████| 18/18 [00:00<00:00, 25.96it/s]\n",
      "12/01/2021 14:19:44 - INFO - mingpt.trainer -   test loss: 0.010690\n",
      "epoch 58 iter 17: train loss 0.18323. lr 5.463996e-04: 100%|██████████| 18/18 [00:00<00:00, 26.72it/s]\n",
      "12/01/2021 14:19:45 - INFO - mingpt.trainer -   test loss: 0.012070\n",
      "epoch 59 iter 17: train loss 0.22029. lr 5.445938e-04: 100%|██████████| 18/18 [00:00<00:00, 25.65it/s]\n",
      "12/01/2021 14:19:47 - INFO - mingpt.trainer -   test loss: 0.013708\n",
      "epoch 60 iter 17: train loss 0.23937. lr 5.427611e-04: 100%|██████████| 18/18 [00:00<00:00, 24.16it/s]\n",
      "12/01/2021 14:19:48 - INFO - mingpt.trainer -   test loss: 0.009883\n",
      "epoch 61 iter 17: train loss 0.15639. lr 5.409018e-04: 100%|██████████| 18/18 [00:00<00:00, 26.59it/s]\n",
      "12/01/2021 14:19:50 - INFO - mingpt.trainer -   test loss: 0.009585\n",
      "epoch 62 iter 17: train loss 0.18476. lr 5.390161e-04: 100%|██████████| 18/18 [00:00<00:00, 26.90it/s]\n",
      "12/01/2021 14:19:51 - INFO - mingpt.trainer -   test loss: 0.009311\n",
      "epoch 63 iter 17: train loss 0.20442. lr 5.371042e-04: 100%|██████████| 18/18 [00:00<00:00, 25.49it/s]\n",
      "12/01/2021 14:19:52 - INFO - mingpt.trainer -   test loss: 0.008979\n",
      "epoch 64 iter 17: train loss 0.16129. lr 5.351663e-04: 100%|██████████| 18/18 [00:00<00:00, 26.37it/s]\n",
      "12/01/2021 14:19:54 - INFO - mingpt.trainer -   test loss: 0.006197\n",
      "epoch 65 iter 17: train loss 0.18324. lr 5.332025e-04: 100%|██████████| 18/18 [00:00<00:00, 26.95it/s]\n",
      "12/01/2021 14:19:55 - INFO - mingpt.trainer -   test loss: 0.008099\n",
      "epoch 66 iter 17: train loss 0.19441. lr 5.312132e-04: 100%|██████████| 18/18 [00:00<00:00, 26.99it/s]\n",
      "12/01/2021 14:19:57 - INFO - mingpt.trainer -   test loss: 0.006827\n",
      "epoch 67 iter 17: train loss 0.18009. lr 5.291985e-04: 100%|██████████| 18/18 [00:00<00:00, 25.36it/s]\n",
      "12/01/2021 14:19:58 - INFO - mingpt.trainer -   test loss: 0.008755\n",
      "epoch 68 iter 17: train loss 0.17252. lr 5.271587e-04: 100%|██████████| 18/18 [00:00<00:00, 26.24it/s]\n",
      "12/01/2021 14:19:59 - INFO - mingpt.trainer -   test loss: 0.006271\n",
      "epoch 69 iter 17: train loss 0.14604. lr 5.250940e-04: 100%|██████████| 18/18 [00:00<00:00, 26.76it/s]\n",
      "12/01/2021 14:20:01 - INFO - mingpt.trainer -   test loss: 0.006685\n",
      "epoch 70 iter 17: train loss 0.20076. lr 5.230046e-04: 100%|██████████| 18/18 [00:00<00:00, 26.19it/s]\n",
      "12/01/2021 14:20:02 - INFO - mingpt.trainer -   test loss: 0.005201\n",
      "epoch 71 iter 17: train loss 0.19102. lr 5.208907e-04: 100%|██████████| 18/18 [00:00<00:00, 25.79it/s]\n",
      "12/01/2021 14:20:04 - INFO - mingpt.trainer -   test loss: 0.006882\n",
      "epoch 72 iter 17: train loss 0.14714. lr 5.187526e-04: 100%|██████████| 18/18 [00:00<00:00, 26.76it/s]\n",
      "12/01/2021 14:20:05 - INFO - mingpt.trainer -   test loss: 0.004625\n",
      "epoch 73 iter 17: train loss 0.13954. lr 5.165905e-04: 100%|██████████| 18/18 [00:00<00:00, 26.66it/s]\n",
      "12/01/2021 14:20:06 - INFO - mingpt.trainer -   test loss: 0.005351\n",
      "epoch 74 iter 17: train loss 0.15722. lr 5.144046e-04: 100%|██████████| 18/18 [00:00<00:00, 26.93it/s]\n",
      "12/01/2021 14:20:08 - INFO - mingpt.trainer -   test loss: 0.004945\n",
      "epoch 75 iter 17: train loss 0.16425. lr 5.121952e-04: 100%|██████████| 18/18 [00:00<00:00, 24.87it/s]\n",
      "12/01/2021 14:20:09 - INFO - mingpt.trainer -   test loss: 0.004415\n",
      "epoch 76 iter 17: train loss 0.17735. lr 5.099626e-04: 100%|██████████| 18/18 [00:00<00:00, 26.16it/s]\n",
      "12/01/2021 14:20:11 - INFO - mingpt.trainer -   test loss: 0.005564\n",
      "epoch 77 iter 17: train loss 0.14099. lr 5.077069e-04: 100%|██████████| 18/18 [00:00<00:00, 26.14it/s]\n",
      "12/01/2021 14:20:12 - INFO - mingpt.trainer -   test loss: 0.004691\n",
      "epoch 78 iter 17: train loss 0.14800. lr 5.054284e-04: 100%|██████████| 18/18 [00:00<00:00, 26.67it/s]\n",
      "12/01/2021 14:20:13 - INFO - mingpt.trainer -   test loss: 0.003512\n",
      "epoch 79 iter 17: train loss 0.14139. lr 5.031274e-04: 100%|██████████| 18/18 [00:00<00:00, 26.48it/s]\n",
      "12/01/2021 14:20:15 - INFO - mingpt.trainer -   test loss: 0.004220\n",
      "epoch 80 iter 17: train loss 0.14820. lr 5.008041e-04: 100%|██████████| 18/18 [00:00<00:00, 25.54it/s]\n",
      "12/01/2021 14:20:16 - INFO - mingpt.trainer -   test loss: 0.002872\n",
      "epoch 81 iter 17: train loss 0.13743. lr 4.984588e-04: 100%|██████████| 18/18 [00:00<00:00, 27.14it/s]\n",
      "12/01/2021 14:20:18 - INFO - mingpt.trainer -   test loss: 0.004280\n",
      "epoch 82 iter 17: train loss 0.15455. lr 4.960917e-04: 100%|██████████| 18/18 [00:00<00:00, 25.44it/s]\n",
      "12/01/2021 14:20:19 - INFO - mingpt.trainer -   test loss: 0.004717\n",
      "epoch 83 iter 17: train loss 0.12651. lr 4.937031e-04: 100%|██████████| 18/18 [00:00<00:00, 26.03it/s]\n",
      "12/01/2021 14:20:20 - INFO - mingpt.trainer -   test loss: 0.003787\n",
      "epoch 84 iter 17: train loss 0.13204. lr 4.912933e-04: 100%|██████████| 18/18 [00:00<00:00, 25.50it/s]\n",
      "12/01/2021 14:20:22 - INFO - mingpt.trainer -   test loss: 0.003229\n",
      "epoch 85 iter 17: train loss 0.13556. lr 4.888625e-04: 100%|██████████| 18/18 [00:00<00:00, 25.15it/s]\n",
      "12/01/2021 14:20:23 - INFO - mingpt.trainer -   test loss: 0.002866\n",
      "epoch 86 iter 17: train loss 0.16185. lr 4.864109e-04: 100%|██████████| 18/18 [00:00<00:00, 26.46it/s]\n",
      "12/01/2021 14:20:25 - INFO - mingpt.trainer -   test loss: 0.003024\n",
      "epoch 87 iter 17: train loss 0.12718. lr 4.839390e-04: 100%|██████████| 18/18 [00:00<00:00, 26.26it/s]\n",
      "12/01/2021 14:20:26 - INFO - mingpt.trainer -   test loss: 0.003581\n",
      "epoch 88 iter 17: train loss 0.14033. lr 4.814468e-04: 100%|██████████| 18/18 [00:00<00:00, 26.14it/s]\n",
      "12/01/2021 14:20:27 - INFO - mingpt.trainer -   test loss: 0.002643\n",
      "epoch 89 iter 17: train loss 0.12724. lr 4.789347e-04: 100%|██████████| 18/18 [00:00<00:00, 26.09it/s]\n",
      "12/01/2021 14:20:29 - INFO - mingpt.trainer -   test loss: 0.002649\n",
      "epoch 90 iter 17: train loss 0.11889. lr 4.764031e-04: 100%|██████████| 18/18 [00:00<00:00, 26.35it/s]\n",
      "12/01/2021 14:20:30 - INFO - mingpt.trainer -   test loss: 0.002933\n",
      "epoch 91 iter 17: train loss 0.14110. lr 4.738520e-04: 100%|██████████| 18/18 [00:00<00:00, 26.04it/s]\n",
      "12/01/2021 14:20:32 - INFO - mingpt.trainer -   test loss: 0.003930\n",
      "epoch 92 iter 17: train loss 0.12313. lr 4.712819e-04: 100%|██████████| 18/18 [00:00<00:00, 26.86it/s]\n",
      "12/01/2021 14:20:33 - INFO - mingpt.trainer -   test loss: 0.004316\n",
      "epoch 93 iter 17: train loss 0.12404. lr 4.686930e-04: 100%|██████████| 18/18 [00:00<00:00, 26.86it/s]\n",
      "12/01/2021 14:20:34 - INFO - mingpt.trainer -   test loss: 0.002663\n",
      "epoch 94 iter 17: train loss 0.12429. lr 4.660856e-04: 100%|██████████| 18/18 [00:00<00:00, 26.59it/s]\n",
      "12/01/2021 14:20:36 - INFO - mingpt.trainer -   test loss: 0.002271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 95 iter 17: train loss 0.12762. lr 4.634600e-04: 100%|██████████| 18/18 [00:00<00:00, 26.73it/s]\n",
      "12/01/2021 14:20:37 - INFO - mingpt.trainer -   test loss: 0.002380\n",
      "epoch 96 iter 17: train loss 0.13238. lr 4.608164e-04: 100%|██████████| 18/18 [00:00<00:00, 25.96it/s]\n",
      "12/01/2021 14:20:39 - INFO - mingpt.trainer -   test loss: 0.001953\n",
      "epoch 97 iter 17: train loss 0.14967. lr 4.581553e-04: 100%|██████████| 18/18 [00:00<00:00, 27.23it/s]\n",
      "12/01/2021 14:20:40 - INFO - mingpt.trainer -   test loss: 0.002322\n",
      "epoch 98 iter 17: train loss 0.13206. lr 4.554767e-04: 100%|██████████| 18/18 [00:00<00:00, 25.83it/s]\n",
      "12/01/2021 14:20:41 - INFO - mingpt.trainer -   test loss: 0.002619\n",
      "epoch 99 iter 17: train loss 0.12263. lr 4.527811e-04: 100%|██████████| 18/18 [00:00<00:00, 26.28it/s]\n",
      "12/01/2021 14:20:43 - INFO - mingpt.trainer -   test loss: 0.002383\n",
      "epoch 100 iter 17: train loss 0.12614. lr 4.500688e-04: 100%|██████████| 18/18 [00:00<00:00, 26.11it/s]\n",
      "12/01/2021 14:20:44 - INFO - mingpt.trainer -   test loss: 0.001904\n",
      "epoch 101 iter 17: train loss 0.11699. lr 4.473400e-04: 100%|██████████| 18/18 [00:00<00:00, 26.16it/s]\n",
      "12/01/2021 14:20:46 - INFO - mingpt.trainer -   test loss: 0.002301\n",
      "epoch 102 iter 17: train loss 0.10522. lr 4.445950e-04: 100%|██████████| 18/18 [00:00<00:00, 25.73it/s]\n",
      "12/01/2021 14:20:47 - INFO - mingpt.trainer -   test loss: 0.001884\n",
      "epoch 103 iter 17: train loss 0.11110. lr 4.418342e-04: 100%|██████████| 18/18 [00:00<00:00, 25.67it/s]\n",
      "12/01/2021 14:20:49 - INFO - mingpt.trainer -   test loss: 0.001986\n",
      "epoch 104 iter 17: train loss 0.11718. lr 4.390578e-04: 100%|██████████| 18/18 [00:00<00:00, 26.43it/s]\n",
      "12/01/2021 14:20:50 - INFO - mingpt.trainer -   test loss: 0.001519\n",
      "epoch 105 iter 17: train loss 0.10841. lr 4.362662e-04: 100%|██████████| 18/18 [00:00<00:00, 26.41it/s]\n",
      "12/01/2021 14:20:51 - INFO - mingpt.trainer -   test loss: 0.001687\n",
      "epoch 106 iter 17: train loss 0.12263. lr 4.334596e-04: 100%|██████████| 18/18 [00:00<00:00, 27.71it/s]\n",
      "12/01/2021 14:20:53 - INFO - mingpt.trainer -   test loss: 0.001681\n",
      "epoch 107 iter 17: train loss 0.09816. lr 4.306383e-04: 100%|██████████| 18/18 [00:00<00:00, 28.01it/s]\n",
      "12/01/2021 14:20:54 - INFO - mingpt.trainer -   test loss: 0.001892\n",
      "epoch 108 iter 17: train loss 0.11059. lr 4.278028e-04: 100%|██████████| 18/18 [00:00<00:00, 26.61it/s]\n",
      "12/01/2021 14:20:55 - INFO - mingpt.trainer -   test loss: 0.002404\n",
      "epoch 109 iter 17: train loss 0.10611. lr 4.249532e-04: 100%|██████████| 18/18 [00:00<00:00, 25.38it/s]\n",
      "12/01/2021 14:20:57 - INFO - mingpt.trainer -   test loss: 0.001649\n",
      "epoch 110 iter 17: train loss 0.10897. lr 4.220899e-04: 100%|██████████| 18/18 [00:00<00:00, 25.38it/s]\n",
      "12/01/2021 14:20:58 - INFO - mingpt.trainer -   test loss: 0.003084\n",
      "epoch 111 iter 17: train loss 0.12169. lr 4.192133e-04: 100%|██████████| 18/18 [00:00<00:00, 27.80it/s]\n",
      "12/01/2021 14:21:00 - INFO - mingpt.trainer -   test loss: 0.002049\n",
      "epoch 112 iter 17: train loss 0.10394. lr 4.163235e-04: 100%|██████████| 18/18 [00:00<00:00, 27.83it/s]\n",
      "12/01/2021 14:21:01 - INFO - mingpt.trainer -   test loss: 0.001209\n",
      "epoch 113 iter 17: train loss 0.11571. lr 4.134210e-04: 100%|██████████| 18/18 [00:00<00:00, 28.16it/s]\n",
      "12/01/2021 14:21:02 - INFO - mingpt.trainer -   test loss: 0.001150\n",
      "epoch 114 iter 17: train loss 0.10240. lr 4.105061e-04: 100%|██████████| 18/18 [00:00<00:00, 26.68it/s]\n",
      "12/01/2021 14:21:04 - INFO - mingpt.trainer -   test loss: 0.001046\n",
      "epoch 115 iter 17: train loss 0.12095. lr 4.075790e-04: 100%|██████████| 18/18 [00:00<00:00, 27.96it/s]\n",
      "12/01/2021 14:21:05 - INFO - mingpt.trainer -   test loss: 0.001540\n",
      "epoch 116 iter 17: train loss 0.09773. lr 4.046401e-04: 100%|██████████| 18/18 [00:00<00:00, 27.29it/s]\n",
      "12/01/2021 14:21:06 - INFO - mingpt.trainer -   test loss: 0.002271\n",
      "epoch 117 iter 17: train loss 0.09046. lr 4.016898e-04: 100%|██████████| 18/18 [00:00<00:00, 25.28it/s]\n",
      "12/01/2021 14:21:08 - INFO - mingpt.trainer -   test loss: 0.001646\n",
      "epoch 118 iter 17: train loss 0.08993. lr 3.987283e-04: 100%|██████████| 18/18 [00:00<00:00, 27.34it/s]\n",
      "12/01/2021 14:21:09 - INFO - mingpt.trainer -   test loss: 0.001238\n",
      "epoch 119 iter 17: train loss 0.08657. lr 3.957559e-04: 100%|██████████| 18/18 [00:00<00:00, 27.24it/s]\n",
      "12/01/2021 14:21:11 - INFO - mingpt.trainer -   test loss: 0.001457\n",
      "epoch 120 iter 17: train loss 0.09986. lr 3.927731e-04: 100%|██████████| 18/18 [00:00<00:00, 27.20it/s]\n",
      "12/01/2021 14:21:12 - INFO - mingpt.trainer -   test loss: 0.001296\n",
      "epoch 121 iter 17: train loss 0.11469. lr 3.897801e-04: 100%|██████████| 18/18 [00:00<00:00, 27.04it/s]\n",
      "12/01/2021 14:21:13 - INFO - mingpt.trainer -   test loss: 0.001228\n",
      "epoch 122 iter 17: train loss 0.09285. lr 3.867772e-04: 100%|██████████| 18/18 [00:00<00:00, 27.00it/s]\n",
      "12/01/2021 14:21:15 - INFO - mingpt.trainer -   test loss: 0.001153\n",
      "epoch 123 iter 17: train loss 0.09817. lr 3.837648e-04: 100%|██████████| 18/18 [00:00<00:00, 27.63it/s]\n",
      "12/01/2021 14:21:16 - INFO - mingpt.trainer -   test loss: 0.000958\n",
      "epoch 124 iter 17: train loss 0.09628. lr 3.807433e-04: 100%|██████████| 18/18 [00:00<00:00, 24.89it/s]\n",
      "12/01/2021 14:21:18 - INFO - mingpt.trainer -   test loss: 0.001151\n",
      "epoch 125 iter 17: train loss 0.08860. lr 3.777129e-04: 100%|██████████| 18/18 [00:00<00:00, 26.33it/s]\n",
      "12/01/2021 14:21:19 - INFO - mingpt.trainer -   test loss: 0.001053\n",
      "epoch 126 iter 17: train loss 0.09639. lr 3.746739e-04: 100%|██████████| 18/18 [00:00<00:00, 26.40it/s]\n",
      "12/01/2021 14:21:20 - INFO - mingpt.trainer -   test loss: 0.001782\n",
      "epoch 127 iter 17: train loss 0.10432. lr 3.716268e-04: 100%|██████████| 18/18 [00:00<00:00, 26.60it/s]\n",
      "12/01/2021 14:21:22 - INFO - mingpt.trainer -   test loss: 0.001240\n",
      "epoch 128 iter 17: train loss 0.08795. lr 3.685718e-04: 100%|██████████| 18/18 [00:00<00:00, 26.15it/s]\n",
      "12/01/2021 14:21:23 - INFO - mingpt.trainer -   test loss: 0.001074\n",
      "epoch 129 iter 17: train loss 0.08426. lr 3.655093e-04: 100%|██████████| 18/18 [00:00<00:00, 25.54it/s]\n",
      "12/01/2021 14:21:25 - INFO - mingpt.trainer -   test loss: 0.000975\n",
      "epoch 130 iter 17: train loss 0.10918. lr 3.624396e-04: 100%|██████████| 18/18 [00:00<00:00, 25.86it/s]\n",
      "12/01/2021 14:21:26 - INFO - mingpt.trainer -   test loss: 0.001128\n",
      "epoch 131 iter 17: train loss 0.08816. lr 3.593630e-04: 100%|██████████| 18/18 [00:00<00:00, 24.18it/s]\n",
      "12/01/2021 14:21:27 - INFO - mingpt.trainer -   test loss: 0.001595\n",
      "epoch 132 iter 17: train loss 0.09893. lr 3.562799e-04: 100%|██████████| 18/18 [00:00<00:00, 26.00it/s]\n",
      "12/01/2021 14:21:29 - INFO - mingpt.trainer -   test loss: 0.001049\n",
      "epoch 133 iter 17: train loss 0.09416. lr 3.531907e-04: 100%|██████████| 18/18 [00:00<00:00, 27.03it/s]\n",
      "12/01/2021 14:21:30 - INFO - mingpt.trainer -   test loss: 0.000895\n",
      "epoch 134 iter 17: train loss 0.07850. lr 3.500956e-04: 100%|██████████| 18/18 [00:00<00:00, 27.87it/s]\n",
      "12/01/2021 14:21:32 - INFO - mingpt.trainer -   test loss: 0.001154\n",
      "epoch 135 iter 17: train loss 0.08321. lr 3.469951e-04: 100%|██████████| 18/18 [00:00<00:00, 28.86it/s]\n",
      "12/01/2021 14:21:33 - INFO - mingpt.trainer -   test loss: 0.001175\n",
      "epoch 136 iter 17: train loss 0.10668. lr 3.438894e-04: 100%|██████████| 18/18 [00:00<00:00, 28.40it/s]\n",
      "12/01/2021 14:21:34 - INFO - mingpt.trainer -   test loss: 0.000893\n",
      "epoch 137 iter 17: train loss 0.08513. lr 3.407788e-04: 100%|██████████| 18/18 [00:00<00:00, 28.16it/s]\n",
      "12/01/2021 14:21:36 - INFO - mingpt.trainer -   test loss: 0.001117\n",
      "epoch 138 iter 17: train loss 0.09412. lr 3.376638e-04: 100%|██████████| 18/18 [00:00<00:00, 25.82it/s]\n",
      "12/01/2021 14:21:37 - INFO - mingpt.trainer -   test loss: 0.001004\n",
      "epoch 139 iter 17: train loss 0.07955. lr 3.345447e-04: 100%|██████████| 18/18 [00:00<00:00, 28.50it/s]\n",
      "12/01/2021 14:21:38 - INFO - mingpt.trainer -   test loss: 0.001314\n",
      "epoch 140 iter 17: train loss 0.10038. lr 3.314217e-04: 100%|██████████| 18/18 [00:00<00:00, 28.48it/s]\n",
      "12/01/2021 14:21:40 - INFO - mingpt.trainer -   test loss: 0.001248\n",
      "epoch 141 iter 17: train loss 0.09222. lr 3.282954e-04: 100%|██████████| 18/18 [00:00<00:00, 26.56it/s]\n",
      "12/01/2021 14:21:41 - INFO - mingpt.trainer -   test loss: 0.000838\n",
      "epoch 142 iter 17: train loss 0.08676. lr 3.251659e-04: 100%|██████████| 18/18 [00:00<00:00, 26.57it/s]\n",
      "12/01/2021 14:21:42 - INFO - mingpt.trainer -   test loss: 0.001014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 143 iter 17: train loss 0.07530. lr 3.220337e-04: 100%|██████████| 18/18 [00:00<00:00, 26.65it/s]\n",
      "12/01/2021 14:21:44 - INFO - mingpt.trainer -   test loss: 0.001086\n",
      "epoch 144 iter 17: train loss 0.10555. lr 3.188990e-04: 100%|██████████| 18/18 [00:00<00:00, 26.70it/s]\n",
      "12/01/2021 14:21:45 - INFO - mingpt.trainer -   test loss: 0.000876\n",
      "epoch 145 iter 17: train loss 0.08584. lr 3.157623e-04: 100%|██████████| 18/18 [00:00<00:00, 22.88it/s]\n",
      "12/01/2021 14:21:47 - INFO - mingpt.trainer -   test loss: 0.001008\n",
      "epoch 146 iter 17: train loss 0.08591. lr 3.126238e-04: 100%|██████████| 18/18 [00:00<00:00, 26.99it/s]\n",
      "12/01/2021 14:21:48 - INFO - mingpt.trainer -   test loss: 0.000870\n",
      "epoch 147 iter 17: train loss 0.08928. lr 3.094840e-04: 100%|██████████| 18/18 [00:00<00:00, 26.58it/s]\n",
      "12/01/2021 14:21:49 - INFO - mingpt.trainer -   test loss: 0.000764\n",
      "epoch 148 iter 17: train loss 0.07990. lr 3.063431e-04: 100%|██████████| 18/18 [00:00<00:00, 26.68it/s]\n",
      "12/01/2021 14:21:51 - INFO - mingpt.trainer -   test loss: 0.000750\n",
      "epoch 149 iter 17: train loss 0.06326. lr 3.032015e-04: 100%|██████████| 18/18 [00:00<00:00, 26.37it/s]\n",
      "12/01/2021 14:21:52 - INFO - mingpt.trainer -   test loss: 0.000609\n",
      "epoch 150 iter 17: train loss 0.08582. lr 3.000596e-04: 100%|██████████| 18/18 [00:00<00:00, 26.82it/s]\n",
      "12/01/2021 14:21:54 - INFO - mingpt.trainer -   test loss: 0.000867\n",
      "epoch 151 iter 17: train loss 0.07669. lr 2.969176e-04: 100%|██████████| 18/18 [00:00<00:00, 26.29it/s]\n",
      "12/01/2021 14:21:55 - INFO - mingpt.trainer -   test loss: 0.000754\n",
      "epoch 152 iter 17: train loss 0.06672. lr 2.937760e-04: 100%|██████████| 18/18 [00:00<00:00, 25.50it/s]\n",
      "12/01/2021 14:21:56 - INFO - mingpt.trainer -   test loss: 0.000881\n",
      "epoch 153 iter 17: train loss 0.08749. lr 2.906351e-04: 100%|██████████| 18/18 [00:00<00:00, 26.63it/s]\n",
      "12/01/2021 14:21:58 - INFO - mingpt.trainer -   test loss: 0.000661\n",
      "epoch 154 iter 17: train loss 0.08474. lr 2.874952e-04: 100%|██████████| 18/18 [00:00<00:00, 27.30it/s]\n",
      "12/01/2021 14:21:59 - INFO - mingpt.trainer -   test loss: 0.002154\n",
      "epoch 155 iter 17: train loss 0.06507. lr 2.843567e-04: 100%|██████████| 18/18 [00:00<00:00, 26.62it/s]\n",
      "12/01/2021 14:22:00 - INFO - mingpt.trainer -   test loss: 0.000822\n",
      "epoch 156 iter 17: train loss 0.06484. lr 2.812199e-04: 100%|██████████| 18/18 [00:00<00:00, 26.46it/s]\n",
      "12/01/2021 14:22:02 - INFO - mingpt.trainer -   test loss: 0.000528\n",
      "epoch 157 iter 17: train loss 0.07474. lr 2.780852e-04: 100%|██████████| 18/18 [00:00<00:00, 26.46it/s]\n",
      "12/01/2021 14:22:03 - INFO - mingpt.trainer -   test loss: 0.000674\n",
      "epoch 158 iter 17: train loss 0.07765. lr 2.749529e-04: 100%|██████████| 18/18 [00:00<00:00, 26.89it/s]\n",
      "12/01/2021 14:22:05 - INFO - mingpt.trainer -   test loss: 0.000710\n",
      "epoch 159 iter 17: train loss 0.09534. lr 2.718233e-04: 100%|██████████| 18/18 [00:00<00:00, 25.25it/s]\n",
      "12/01/2021 14:22:06 - INFO - mingpt.trainer -   test loss: 0.000800\n",
      "epoch 160 iter 17: train loss 0.07519. lr 2.686968e-04: 100%|██████████| 18/18 [00:00<00:00, 25.80it/s]\n",
      "12/01/2021 14:22:08 - INFO - mingpt.trainer -   test loss: 0.000731\n",
      "epoch 161 iter 17: train loss 0.07183. lr 2.655737e-04: 100%|██████████| 18/18 [00:00<00:00, 26.40it/s]\n",
      "12/01/2021 14:22:09 - INFO - mingpt.trainer -   test loss: 0.000575\n",
      "epoch 162 iter 17: train loss 0.08415. lr 2.624544e-04: 100%|██████████| 18/18 [00:00<00:00, 26.19it/s]\n",
      "12/01/2021 14:22:10 - INFO - mingpt.trainer -   test loss: 0.000612\n",
      "epoch 163 iter 17: train loss 0.07294. lr 2.593392e-04: 100%|██████████| 18/18 [00:00<00:00, 26.30it/s]\n",
      "12/01/2021 14:22:12 - INFO - mingpt.trainer -   test loss: 0.000494\n",
      "epoch 164 iter 17: train loss 0.06820. lr 2.562285e-04: 100%|██████████| 18/18 [00:00<00:00, 25.78it/s]\n",
      "12/01/2021 14:22:13 - INFO - mingpt.trainer -   test loss: 0.000492\n",
      "epoch 165 iter 17: train loss 0.07274. lr 2.531226e-04: 100%|██████████| 18/18 [00:00<00:00, 26.62it/s]\n",
      "12/01/2021 14:22:14 - INFO - mingpt.trainer -   test loss: 0.000640\n",
      "epoch 166 iter 17: train loss 0.07763. lr 2.500219e-04: 100%|██████████| 18/18 [00:00<00:00, 25.96it/s]\n",
      "12/01/2021 14:22:16 - INFO - mingpt.trainer -   test loss: 0.000687\n",
      "epoch 167 iter 17: train loss 0.06450. lr 2.469266e-04: 100%|██████████| 18/18 [00:00<00:00, 26.80it/s]\n",
      "12/01/2021 14:22:17 - INFO - mingpt.trainer -   test loss: 0.000513\n",
      "epoch 168 iter 17: train loss 0.07868. lr 2.438371e-04: 100%|██████████| 18/18 [00:00<00:00, 26.88it/s]\n",
      "12/01/2021 14:22:19 - INFO - mingpt.trainer -   test loss: 0.000458\n",
      "epoch 169 iter 17: train loss 0.08196. lr 2.407538e-04: 100%|██████████| 18/18 [00:00<00:00, 28.16it/s]\n",
      "12/01/2021 14:22:20 - INFO - mingpt.trainer -   test loss: 0.000547\n",
      "epoch 170 iter 17: train loss 0.08700. lr 2.376770e-04: 100%|██████████| 18/18 [00:00<00:00, 27.17it/s]\n",
      "12/01/2021 14:22:21 - INFO - mingpt.trainer -   test loss: 0.000457\n",
      "epoch 171 iter 17: train loss 0.07443. lr 2.346070e-04: 100%|██████████| 18/18 [00:00<00:00, 27.96it/s]\n",
      "12/01/2021 14:22:23 - INFO - mingpt.trainer -   test loss: 0.000566\n",
      "epoch 172 iter 17: train loss 0.07340. lr 2.315442e-04: 100%|██████████| 18/18 [00:00<00:00, 26.96it/s]\n",
      "12/01/2021 14:22:24 - INFO - mingpt.trainer -   test loss: 0.000467\n",
      "epoch 173 iter 17: train loss 0.08256. lr 2.284890e-04: 100%|██████████| 18/18 [00:00<00:00, 26.19it/s]\n",
      "12/01/2021 14:22:26 - INFO - mingpt.trainer -   test loss: 0.000417\n",
      "epoch 174 iter 17: train loss 0.07531. lr 2.254415e-04: 100%|██████████| 18/18 [00:00<00:00, 24.68it/s]\n",
      "12/01/2021 14:22:27 - INFO - mingpt.trainer -   test loss: 0.000384\n",
      "epoch 175 iter 17: train loss 0.07815. lr 2.224022e-04: 100%|██████████| 18/18 [00:00<00:00, 26.80it/s]\n",
      "12/01/2021 14:22:28 - INFO - mingpt.trainer -   test loss: 0.000400\n",
      "epoch 176 iter 17: train loss 0.07105. lr 2.193715e-04: 100%|██████████| 18/18 [00:00<00:00, 27.74it/s]\n",
      "12/01/2021 14:22:30 - INFO - mingpt.trainer -   test loss: 0.000465\n",
      "epoch 177 iter 17: train loss 0.06334. lr 2.163496e-04: 100%|██████████| 18/18 [00:00<00:00, 27.39it/s]\n",
      "12/01/2021 14:22:31 - INFO - mingpt.trainer -   test loss: 0.000412\n",
      "epoch 178 iter 17: train loss 0.06393. lr 2.133369e-04: 100%|██████████| 18/18 [00:00<00:00, 28.11it/s]\n",
      "12/01/2021 14:22:32 - INFO - mingpt.trainer -   test loss: 0.000378\n",
      "epoch 179 iter 17: train loss 0.07039. lr 2.103336e-04: 100%|██████████| 18/18 [00:00<00:00, 27.46it/s]\n",
      "12/01/2021 14:22:34 - INFO - mingpt.trainer -   test loss: 0.000471\n",
      "epoch 180 iter 17: train loss 0.07095. lr 2.073402e-04: 100%|██████████| 18/18 [00:00<00:00, 26.40it/s]\n",
      "12/01/2021 14:22:35 - INFO - mingpt.trainer -   test loss: 0.000368\n",
      "epoch 181 iter 17: train loss 0.07519. lr 2.043570e-04: 100%|██████████| 18/18 [00:00<00:00, 24.69it/s]\n",
      "12/01/2021 14:22:37 - INFO - mingpt.trainer -   test loss: 0.000405\n",
      "epoch 182 iter 17: train loss 0.06812. lr 2.013843e-04: 100%|██████████| 18/18 [00:00<00:00, 27.37it/s]\n",
      "12/01/2021 14:22:38 - INFO - mingpt.trainer -   test loss: 0.000458\n",
      "epoch 183 iter 17: train loss 0.07278. lr 1.984224e-04: 100%|██████████| 18/18 [00:00<00:00, 27.75it/s]\n",
      "12/01/2021 14:22:39 - INFO - mingpt.trainer -   test loss: 0.000464\n",
      "epoch 184 iter 17: train loss 0.06274. lr 1.954716e-04: 100%|██████████| 18/18 [00:00<00:00, 27.52it/s]\n",
      "12/01/2021 14:22:41 - INFO - mingpt.trainer -   test loss: 0.000617\n",
      "epoch 185 iter 17: train loss 0.07123. lr 1.925323e-04: 100%|██████████| 18/18 [00:00<00:00, 27.63it/s]\n",
      "12/01/2021 14:22:42 - INFO - mingpt.trainer -   test loss: 0.000319\n",
      "epoch 186 iter 17: train loss 0.08403. lr 1.896047e-04: 100%|██████████| 18/18 [00:00<00:00, 26.96it/s]\n",
      "12/01/2021 14:22:44 - INFO - mingpt.trainer -   test loss: 0.000398\n",
      "epoch 187 iter 17: train loss 0.07734. lr 1.866893e-04: 100%|██████████| 18/18 [00:00<00:00, 28.41it/s]\n",
      "12/01/2021 14:22:45 - INFO - mingpt.trainer -   test loss: 0.000434\n",
      "epoch 188 iter 17: train loss 0.06708. lr 1.837863e-04: 100%|██████████| 18/18 [00:00<00:00, 27.96it/s]\n",
      "12/01/2021 14:22:46 - INFO - mingpt.trainer -   test loss: 0.000367\n",
      "epoch 189 iter 17: train loss 0.07298. lr 1.808961e-04: 100%|██████████| 18/18 [00:00<00:00, 24.06it/s]\n",
      "12/01/2021 14:22:48 - INFO - mingpt.trainer -   test loss: 0.000356\n",
      "epoch 190 iter 17: train loss 0.06872. lr 1.780189e-04: 100%|██████████| 18/18 [00:00<00:00, 28.39it/s]\n",
      "12/01/2021 14:22:49 - INFO - mingpt.trainer -   test loss: 0.000369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 191 iter 17: train loss 0.07119. lr 1.751551e-04: 100%|██████████| 18/18 [00:00<00:00, 26.99it/s]\n",
      "12/01/2021 14:22:50 - INFO - mingpt.trainer -   test loss: 0.000328\n",
      "epoch 192 iter 17: train loss 0.05852. lr 1.723050e-04: 100%|██████████| 18/18 [00:00<00:00, 26.14it/s]\n",
      "12/01/2021 14:22:52 - INFO - mingpt.trainer -   test loss: 0.000310\n",
      "epoch 193 iter 17: train loss 0.06385. lr 1.694689e-04: 100%|██████████| 18/18 [00:00<00:00, 25.83it/s]\n",
      "12/01/2021 14:22:53 - INFO - mingpt.trainer -   test loss: 0.000370\n",
      "epoch 194 iter 17: train loss 0.05569. lr 1.666472e-04: 100%|██████████| 18/18 [00:00<00:00, 27.14it/s]\n",
      "12/01/2021 14:22:55 - INFO - mingpt.trainer -   test loss: 0.000378\n",
      "epoch 195 iter 17: train loss 0.08377. lr 1.638400e-04: 100%|██████████| 18/18 [00:00<00:00, 26.52it/s]\n",
      "12/01/2021 14:22:56 - INFO - mingpt.trainer -   test loss: 0.000313\n",
      "epoch 196 iter 17: train loss 0.07272. lr 1.610478e-04: 100%|██████████| 18/18 [00:00<00:00, 24.33it/s]\n",
      "12/01/2021 14:22:57 - INFO - mingpt.trainer -   test loss: 0.000318\n",
      "epoch 197 iter 17: train loss 0.06840. lr 1.582708e-04: 100%|██████████| 18/18 [00:00<00:00, 25.37it/s]\n",
      "12/01/2021 14:22:59 - INFO - mingpt.trainer -   test loss: 0.000337\n",
      "epoch 198 iter 17: train loss 0.06726. lr 1.555094e-04: 100%|██████████| 18/18 [00:00<00:00, 25.82it/s]\n",
      "12/01/2021 14:23:00 - INFO - mingpt.trainer -   test loss: 0.000266\n",
      "epoch 199 iter 17: train loss 0.06119. lr 1.527638e-04: 100%|██████████| 18/18 [00:00<00:00, 26.13it/s]\n",
      "12/01/2021 14:23:02 - INFO - mingpt.trainer -   test loss: 0.000387\n",
      "epoch 200 iter 17: train loss 0.06303. lr 1.500344e-04: 100%|██████████| 18/18 [00:00<00:00, 25.45it/s]\n",
      "12/01/2021 14:23:03 - INFO - mingpt.trainer -   test loss: 0.000247\n",
      "epoch 201 iter 17: train loss 0.07103. lr 1.473214e-04: 100%|██████████| 18/18 [00:00<00:00, 27.23it/s]\n",
      "12/01/2021 14:23:05 - INFO - mingpt.trainer -   test loss: 0.000300\n",
      "epoch 202 iter 17: train loss 0.08218. lr 1.446252e-04: 100%|██████████| 18/18 [00:00<00:00, 26.39it/s]\n",
      "12/01/2021 14:23:06 - INFO - mingpt.trainer -   test loss: 0.000280\n",
      "epoch 203 iter 17: train loss 0.07267. lr 1.419460e-04: 100%|██████████| 18/18 [00:00<00:00, 24.06it/s]\n",
      "12/01/2021 14:23:07 - INFO - mingpt.trainer -   test loss: 0.000299\n",
      "epoch 204 iter 17: train loss 0.06176. lr 1.392842e-04: 100%|██████████| 18/18 [00:00<00:00, 26.42it/s]\n",
      "12/01/2021 14:23:09 - INFO - mingpt.trainer -   test loss: 0.000300\n",
      "epoch 205 iter 17: train loss 0.06263. lr 1.366399e-04: 100%|██████████| 18/18 [00:00<00:00, 26.39it/s]\n",
      "12/01/2021 14:23:10 - INFO - mingpt.trainer -   test loss: 0.000260\n",
      "epoch 206 iter 17: train loss 0.07262. lr 1.340136e-04: 100%|██████████| 18/18 [00:00<00:00, 26.39it/s]\n",
      "12/01/2021 14:23:12 - INFO - mingpt.trainer -   test loss: 0.000297\n",
      "epoch 207 iter 17: train loss 0.06723. lr 1.314055e-04: 100%|██████████| 18/18 [00:00<00:00, 26.25it/s]\n",
      "12/01/2021 14:23:13 - INFO - mingpt.trainer -   test loss: 0.000259\n",
      "epoch 208 iter 17: train loss 0.06223. lr 1.288159e-04: 100%|██████████| 18/18 [00:00<00:00, 26.93it/s]\n",
      "12/01/2021 14:23:14 - INFO - mingpt.trainer -   test loss: 0.000251\n",
      "epoch 209 iter 17: train loss 0.05472. lr 1.262451e-04: 100%|██████████| 18/18 [00:00<00:00, 26.29it/s]\n",
      "12/01/2021 14:23:16 - INFO - mingpt.trainer -   test loss: 0.000225\n",
      "epoch 210 iter 17: train loss 0.05338. lr 1.236933e-04: 100%|██████████| 18/18 [00:00<00:00, 27.05it/s]\n",
      "12/01/2021 14:23:17 - INFO - mingpt.trainer -   test loss: 0.000239\n",
      "epoch 211 iter 17: train loss 0.06755. lr 1.211609e-04: 100%|██████████| 18/18 [00:00<00:00, 23.17it/s]\n",
      "12/01/2021 14:23:19 - INFO - mingpt.trainer -   test loss: 0.000205\n",
      "epoch 212 iter 17: train loss 0.07457. lr 1.186481e-04: 100%|██████████| 18/18 [00:00<00:00, 27.69it/s]\n",
      "12/01/2021 14:23:20 - INFO - mingpt.trainer -   test loss: 0.000263\n",
      "epoch 213 iter 17: train loss 0.05721. lr 1.161552e-04: 100%|██████████| 18/18 [00:00<00:00, 26.48it/s]\n",
      "12/01/2021 14:23:21 - INFO - mingpt.trainer -   test loss: 0.000241\n",
      "epoch 214 iter 17: train loss 0.06116. lr 1.136824e-04: 100%|██████████| 18/18 [00:00<00:00, 27.20it/s]\n",
      "12/01/2021 14:23:23 - INFO - mingpt.trainer -   test loss: 0.000257\n",
      "epoch 215 iter 17: train loss 0.05574. lr 1.112301e-04: 100%|██████████| 18/18 [00:00<00:00, 26.49it/s]\n",
      "12/01/2021 14:23:24 - INFO - mingpt.trainer -   test loss: 0.000224\n",
      "epoch 216 iter 17: train loss 0.06499. lr 1.087985e-04: 100%|██████████| 18/18 [00:00<00:00, 27.42it/s]\n",
      "12/01/2021 14:23:26 - INFO - mingpt.trainer -   test loss: 0.000241\n",
      "epoch 217 iter 17: train loss 0.06004. lr 1.063879e-04: 100%|██████████| 18/18 [00:00<00:00, 25.50it/s]\n",
      "12/01/2021 14:23:27 - INFO - mingpt.trainer -   test loss: 0.000252\n",
      "epoch 218 iter 17: train loss 0.05864. lr 1.039985e-04: 100%|██████████| 18/18 [00:00<00:00, 24.01it/s]\n",
      "12/01/2021 14:23:28 - INFO - mingpt.trainer -   test loss: 0.000276\n",
      "epoch 219 iter 17: train loss 0.05417. lr 1.016306e-04: 100%|██████████| 18/18 [00:00<00:00, 26.23it/s]\n",
      "12/01/2021 14:23:30 - INFO - mingpt.trainer -   test loss: 0.000222\n",
      "epoch 220 iter 17: train loss 0.05817. lr 9.928443e-05: 100%|██████████| 18/18 [00:00<00:00, 26.27it/s]\n",
      "12/01/2021 14:23:31 - INFO - mingpt.trainer -   test loss: 0.000281\n",
      "epoch 221 iter 17: train loss 0.05773. lr 9.696031e-05: 100%|██████████| 18/18 [00:00<00:00, 26.77it/s]\n",
      "12/01/2021 14:23:33 - INFO - mingpt.trainer -   test loss: 0.000214\n",
      "epoch 222 iter 17: train loss 0.06028. lr 9.465845e-05: 100%|██████████| 18/18 [00:00<00:00, 26.98it/s]\n",
      "12/01/2021 14:23:34 - INFO - mingpt.trainer -   test loss: 0.000231\n",
      "epoch 223 iter 17: train loss 0.05698. lr 9.237912e-05: 100%|██████████| 18/18 [00:00<00:00, 26.47it/s]\n",
      "12/01/2021 14:23:35 - INFO - mingpt.trainer -   test loss: 0.000218\n",
      "epoch 224 iter 17: train loss 0.07044. lr 9.012257e-05: 100%|██████████| 18/18 [00:00<00:00, 26.36it/s]\n",
      "12/01/2021 14:23:37 - INFO - mingpt.trainer -   test loss: 0.000224\n",
      "epoch 225 iter 17: train loss 0.07273. lr 8.788903e-05: 100%|██████████| 18/18 [00:00<00:00, 25.42it/s]\n",
      "12/01/2021 14:23:38 - INFO - mingpt.trainer -   test loss: 0.000206\n",
      "epoch 226 iter 17: train loss 0.05228. lr 8.567876e-05: 100%|██████████| 18/18 [00:00<00:00, 26.58it/s]\n",
      "12/01/2021 14:23:40 - INFO - mingpt.trainer -   test loss: 0.000209\n",
      "epoch 227 iter 17: train loss 0.07326. lr 8.349200e-05: 100%|██████████| 18/18 [00:00<00:00, 26.59it/s]\n",
      "12/01/2021 14:23:41 - INFO - mingpt.trainer -   test loss: 0.000216\n",
      "epoch 228 iter 17: train loss 0.07163. lr 8.132899e-05: 100%|██████████| 18/18 [00:00<00:00, 27.02it/s]\n",
      "12/01/2021 14:23:42 - INFO - mingpt.trainer -   test loss: 0.000208\n",
      "epoch 229 iter 17: train loss 0.04962. lr 7.918996e-05: 100%|██████████| 18/18 [00:00<00:00, 28.89it/s]\n",
      "12/01/2021 14:23:44 - INFO - mingpt.trainer -   test loss: 0.000214\n",
      "epoch 230 iter 17: train loss 0.05460. lr 7.707516e-05: 100%|██████████| 18/18 [00:00<00:00, 26.17it/s]\n",
      "12/01/2021 14:23:45 - INFO - mingpt.trainer -   test loss: 0.000191\n",
      "epoch 231 iter 17: train loss 0.06193. lr 7.498480e-05: 100%|██████████| 18/18 [00:00<00:00, 25.61it/s]\n",
      "12/01/2021 14:23:47 - INFO - mingpt.trainer -   test loss: 0.000230\n",
      "epoch 232 iter 17: train loss 0.05698. lr 7.291913e-05: 100%|██████████| 18/18 [00:00<00:00, 24.34it/s]\n",
      "12/01/2021 14:23:48 - INFO - mingpt.trainer -   test loss: 0.000189\n",
      "epoch 233 iter 17: train loss 0.05424. lr 7.087837e-05: 100%|██████████| 18/18 [00:00<00:00, 28.60it/s]\n",
      "12/01/2021 14:23:49 - INFO - mingpt.trainer -   test loss: 0.000193\n",
      "epoch 234 iter 17: train loss 0.05059. lr 6.886274e-05: 100%|██████████| 18/18 [00:00<00:00, 28.04it/s]\n",
      "12/01/2021 14:23:51 - INFO - mingpt.trainer -   test loss: 0.000166\n",
      "epoch 235 iter 17: train loss 0.06310. lr 6.687246e-05: 100%|██████████| 18/18 [00:00<00:00, 26.66it/s]\n",
      "12/01/2021 14:23:52 - INFO - mingpt.trainer -   test loss: 0.000168\n",
      "epoch 236 iter 17: train loss 0.07142. lr 6.490775e-05: 100%|██████████| 18/18 [00:00<00:00, 25.91it/s]\n",
      "12/01/2021 14:23:53 - INFO - mingpt.trainer -   test loss: 0.000193\n",
      "epoch 237 iter 17: train loss 0.05429. lr 6.296883e-05: 100%|██████████| 18/18 [00:00<00:00, 26.84it/s]\n",
      "12/01/2021 14:23:55 - INFO - mingpt.trainer -   test loss: 0.000204\n",
      "epoch 238 iter 17: train loss 0.05547. lr 6.105591e-05: 100%|██████████| 18/18 [00:00<00:00, 25.26it/s]\n",
      "12/01/2021 14:23:56 - INFO - mingpt.trainer -   test loss: 0.000202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 239 iter 17: train loss 0.05894. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.65it/s]\n",
      "12/01/2021 14:23:58 - INFO - mingpt.trainer -   test loss: 0.000190\n",
      "epoch 240 iter 17: train loss 0.05932. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.69it/s]\n",
      "12/01/2021 14:23:59 - INFO - mingpt.trainer -   test loss: 0.000198\n",
      "epoch 241 iter 17: train loss 0.06459. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.31it/s]\n",
      "12/01/2021 14:24:01 - INFO - mingpt.trainer -   test loss: 0.000194\n",
      "epoch 242 iter 17: train loss 0.05445. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.17it/s]\n",
      "12/01/2021 14:24:02 - INFO - mingpt.trainer -   test loss: 0.000205\n",
      "epoch 243 iter 17: train loss 0.06977. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.06it/s]\n",
      "12/01/2021 14:24:03 - INFO - mingpt.trainer -   test loss: 0.000175\n",
      "epoch 244 iter 17: train loss 0.05742. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.07it/s]\n",
      "12/01/2021 14:24:05 - INFO - mingpt.trainer -   test loss: 0.000159\n",
      "epoch 245 iter 17: train loss 0.05337. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.78it/s]\n",
      "12/01/2021 14:24:06 - INFO - mingpt.trainer -   test loss: 0.000171\n",
      "epoch 246 iter 17: train loss 0.05684. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.02it/s]\n",
      "12/01/2021 14:24:07 - INFO - mingpt.trainer -   test loss: 0.000173\n",
      "epoch 247 iter 17: train loss 0.05311. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 23.46it/s]\n",
      "12/01/2021 14:24:09 - INFO - mingpt.trainer -   test loss: 0.000157\n",
      "epoch 248 iter 17: train loss 0.05921. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.77it/s]\n",
      "12/01/2021 14:24:10 - INFO - mingpt.trainer -   test loss: 0.000198\n",
      "epoch 249 iter 17: train loss 0.05457. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.62it/s]\n",
      "12/01/2021 14:24:12 - INFO - mingpt.trainer -   test loss: 0.000165\n",
      "epoch 250 iter 17: train loss 0.06303. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.00it/s]\n",
      "12/01/2021 14:24:13 - INFO - mingpt.trainer -   test loss: 0.000180\n",
      "epoch 251 iter 17: train loss 0.05424. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.20it/s]\n",
      "12/01/2021 14:24:15 - INFO - mingpt.trainer -   test loss: 0.000167\n",
      "epoch 252 iter 17: train loss 0.06584. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.70it/s]\n",
      "12/01/2021 14:24:16 - INFO - mingpt.trainer -   test loss: 0.000160\n",
      "epoch 253 iter 17: train loss 0.06619. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.93it/s]\n",
      "12/01/2021 14:24:17 - INFO - mingpt.trainer -   test loss: 0.000185\n",
      "epoch 254 iter 17: train loss 0.05129. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.97it/s]\n",
      "12/01/2021 14:24:19 - INFO - mingpt.trainer -   test loss: 0.000174\n",
      "epoch 255 iter 17: train loss 0.06122. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.89it/s]\n",
      "12/01/2021 14:24:20 - INFO - mingpt.trainer -   test loss: 0.000167\n",
      "epoch 256 iter 17: train loss 0.05048. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.07it/s]\n",
      "12/01/2021 14:24:22 - INFO - mingpt.trainer -   test loss: 0.000173\n",
      "epoch 257 iter 17: train loss 0.05524. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.47it/s]\n",
      "12/01/2021 14:24:23 - INFO - mingpt.trainer -   test loss: 0.000193\n",
      "epoch 258 iter 17: train loss 0.05978. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.16it/s]\n",
      "12/01/2021 14:24:25 - INFO - mingpt.trainer -   test loss: 0.000162\n",
      "epoch 259 iter 17: train loss 0.07143. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.51it/s]\n",
      "12/01/2021 14:24:26 - INFO - mingpt.trainer -   test loss: 0.000185\n",
      "epoch 260 iter 17: train loss 0.06880. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.83it/s]\n",
      "12/01/2021 14:24:27 - INFO - mingpt.trainer -   test loss: 0.000176\n",
      "epoch 261 iter 17: train loss 0.06507. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.27it/s]\n",
      "12/01/2021 14:24:29 - INFO - mingpt.trainer -   test loss: 0.000183\n",
      "epoch 262 iter 17: train loss 0.04992. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.23it/s]\n",
      "12/01/2021 14:24:30 - INFO - mingpt.trainer -   test loss: 0.000163\n",
      "epoch 263 iter 17: train loss 0.06377. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.64it/s]\n",
      "12/01/2021 14:24:32 - INFO - mingpt.trainer -   test loss: 0.000191\n",
      "epoch 264 iter 17: train loss 0.06344. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.74it/s]\n",
      "12/01/2021 14:24:33 - INFO - mingpt.trainer -   test loss: 0.000166\n",
      "epoch 265 iter 17: train loss 0.06396. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.49it/s]\n",
      "12/01/2021 14:24:34 - INFO - mingpt.trainer -   test loss: 0.000177\n",
      "epoch 266 iter 17: train loss 0.05110. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.11it/s]\n",
      "12/01/2021 14:24:36 - INFO - mingpt.trainer -   test loss: 0.000168\n",
      "epoch 267 iter 17: train loss 0.05170. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.40it/s]\n",
      "12/01/2021 14:24:37 - INFO - mingpt.trainer -   test loss: 0.000163\n",
      "epoch 268 iter 17: train loss 0.07268. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.25it/s]\n",
      "12/01/2021 14:24:39 - INFO - mingpt.trainer -   test loss: 0.000156\n",
      "epoch 269 iter 17: train loss 0.06398. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.36it/s]\n",
      "12/01/2021 14:24:40 - INFO - mingpt.trainer -   test loss: 0.000162\n",
      "epoch 270 iter 17: train loss 0.06929. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.96it/s]\n",
      "12/01/2021 14:24:41 - INFO - mingpt.trainer -   test loss: 0.000151\n",
      "epoch 271 iter 17: train loss 0.04946. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.63it/s]\n",
      "12/01/2021 14:24:43 - INFO - mingpt.trainer -   test loss: 0.000144\n",
      "epoch 272 iter 17: train loss 0.06018. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.03it/s]\n",
      "12/01/2021 14:24:44 - INFO - mingpt.trainer -   test loss: 0.000175\n",
      "epoch 273 iter 17: train loss 0.05859. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.23it/s]\n",
      "12/01/2021 14:24:46 - INFO - mingpt.trainer -   test loss: 0.000149\n",
      "epoch 274 iter 17: train loss 0.06770. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.67it/s]\n",
      "12/01/2021 14:24:47 - INFO - mingpt.trainer -   test loss: 0.000155\n",
      "epoch 275 iter 17: train loss 0.05755. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 22.98it/s]\n",
      "12/01/2021 14:24:49 - INFO - mingpt.trainer -   test loss: 0.000156\n",
      "epoch 276 iter 17: train loss 0.05443. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.28it/s]\n",
      "12/01/2021 14:24:50 - INFO - mingpt.trainer -   test loss: 0.000183\n",
      "epoch 277 iter 17: train loss 0.06486. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.14it/s]\n",
      "12/01/2021 14:24:51 - INFO - mingpt.trainer -   test loss: 0.000165\n",
      "epoch 278 iter 17: train loss 0.06292. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.25it/s]\n",
      "12/01/2021 14:24:53 - INFO - mingpt.trainer -   test loss: 0.000166\n",
      "epoch 279 iter 17: train loss 0.06071. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.22it/s]\n",
      "12/01/2021 14:24:54 - INFO - mingpt.trainer -   test loss: 0.000179\n",
      "epoch 280 iter 17: train loss 0.05883. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.13it/s]\n",
      "12/01/2021 14:24:56 - INFO - mingpt.trainer -   test loss: 0.000156\n",
      "epoch 281 iter 17: train loss 0.05663. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 28.19it/s]\n",
      "12/01/2021 14:24:57 - INFO - mingpt.trainer -   test loss: 0.000205\n",
      "epoch 282 iter 17: train loss 0.06299. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.15it/s]\n",
      "12/01/2021 14:24:58 - INFO - mingpt.trainer -   test loss: 0.000180\n",
      "epoch 283 iter 17: train loss 0.06344. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.84it/s]\n",
      "12/01/2021 14:25:00 - INFO - mingpt.trainer -   test loss: 0.000184\n",
      "epoch 284 iter 17: train loss 0.05005. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.98it/s]\n",
      "12/01/2021 14:25:01 - INFO - mingpt.trainer -   test loss: 0.000153\n",
      "epoch 285 iter 17: train loss 0.04817. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.58it/s]\n",
      "12/01/2021 14:25:02 - INFO - mingpt.trainer -   test loss: 0.000169\n",
      "epoch 286 iter 17: train loss 0.05729. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.71it/s]\n",
      "12/01/2021 14:25:04 - INFO - mingpt.trainer -   test loss: 0.000172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 287 iter 17: train loss 0.06108. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.51it/s]\n",
      "12/01/2021 14:25:05 - INFO - mingpt.trainer -   test loss: 0.000168\n",
      "epoch 288 iter 17: train loss 0.05117. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.68it/s]\n",
      "12/01/2021 14:25:07 - INFO - mingpt.trainer -   test loss: 0.000158\n",
      "epoch 289 iter 17: train loss 0.05335. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.20it/s]\n",
      "12/01/2021 14:25:08 - INFO - mingpt.trainer -   test loss: 0.000174\n",
      "epoch 290 iter 17: train loss 0.07408. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.35it/s]\n",
      "12/01/2021 14:25:10 - INFO - mingpt.trainer -   test loss: 0.000172\n",
      "epoch 291 iter 17: train loss 0.06583. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.92it/s]\n",
      "12/01/2021 14:25:11 - INFO - mingpt.trainer -   test loss: 0.000166\n",
      "epoch 292 iter 17: train loss 0.05550. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.02it/s]\n",
      "12/01/2021 14:25:12 - INFO - mingpt.trainer -   test loss: 0.000163\n",
      "epoch 293 iter 17: train loss 0.04809. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.18it/s]\n",
      "12/01/2021 14:25:14 - INFO - mingpt.trainer -   test loss: 0.000178\n",
      "epoch 294 iter 17: train loss 0.05769. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.72it/s]\n",
      "12/01/2021 14:25:15 - INFO - mingpt.trainer -   test loss: 0.000162\n",
      "epoch 295 iter 17: train loss 0.06114. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.05it/s]\n",
      "12/01/2021 14:25:17 - INFO - mingpt.trainer -   test loss: 0.000150\n",
      "epoch 296 iter 17: train loss 0.05428. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.13it/s]\n",
      "12/01/2021 14:25:18 - INFO - mingpt.trainer -   test loss: 0.000161\n",
      "epoch 297 iter 17: train loss 0.05659. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.97it/s]\n",
      "12/01/2021 14:25:19 - INFO - mingpt.trainer -   test loss: 0.000157\n",
      "epoch 298 iter 17: train loss 0.05869. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 27.17it/s]\n",
      "12/01/2021 14:25:21 - INFO - mingpt.trainer -   test loss: 0.000147\n",
      "epoch 299 iter 17: train loss 0.05358. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.30it/s]\n",
      "12/01/2021 14:25:22 - INFO - mingpt.trainer -   test loss: 0.000160\n",
      "epoch 300 iter 17: train loss 0.05874. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 26.96it/s]\n",
      "12/01/2021 14:25:24 - INFO - mingpt.trainer -   test loss: 0.000175\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "epochs=300\n",
    "tconf = TrainerConfig(max_epochs=epochs, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=epochs*len(train_dataset)*(ndigit+1),\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import sample\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        d1d2 = x[:, :ndigit*2]\n",
    "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
    "        # decode the integers from individual digits\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "        d3i_pred = (d3 * factors).sum(1)\n",
    "        d3i_gt = d1i + d2i\n",
    "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
    "            if not correct[i]:\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 9000/9000 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "# training set: how well did we memorize?\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 1000/1000 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "# test set: how well did we generalize?\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that's amusing... our model learned everything except 55 + 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mingpt]",
   "language": "python",
   "name": "conda-env-mingpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
